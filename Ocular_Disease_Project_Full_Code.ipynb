{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y5Y_m4_Zqvel"
      ],
      "authorship_tag": "ABX9TyNX1dpt4X/9P60BrA4TW7pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PradhyumnaPrakash/Ocular-Disease-Classification-and-Clustering-/blob/main/Ocular_Disease_Project_Full_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning\n"
      ],
      "metadata": {
        "id": "Y5Y_m4_Zqvel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 1 - Preprocessing**"
      ],
      "metadata": {
        "id": "T2AQg1thrGik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrFxUpQkfYI",
        "outputId": "91518942-56b0-41e7-f05a-9321da335186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To merge all data into one folder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# paths\n",
        "train_dir = \"/content/drive/My Drive/Ocular Disease Dataset 1/Balanced Training Data\"\n",
        "test_dir = \"/content/drive/My Drive/Ocular Disease Dataset 1/Balanced Training Data\"\n",
        "output_dir = \"/content/drive/My Drive/Ocular Disease Dataset 1/Balanced Training Data\"\n",
        "\n",
        "# make sure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# get list of class folders from training (same in testing)\n",
        "classes = os.listdir(train_dir)\n",
        "\n",
        "for cls in classes:\n",
        "    print(f\"Processing {cls}...\")\n",
        "\n",
        "    # create destination subfolder\n",
        "    dest_folder = os.path.join(output_dir, cls)\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "    # source folders\n",
        "    train_folder = os.path.join(train_dir, cls)\n",
        "    test_folder = os.path.join(test_dir, cls)\n",
        "\n",
        "    # combine both sources\n",
        "    for folder in [train_folder, test_folder]:\n",
        "        if not os.path.exists(folder):\n",
        "            continue\n",
        "        for file in os.listdir(folder):\n",
        "            src_path = os.path.join(folder, file)\n",
        "            dest_path = os.path.join(dest_folder, file)\n",
        "\n",
        "            # if duplicate filename, rename before moving\n",
        "            if os.path.exists(dest_path):\n",
        "                base, ext = os.path.splitext(file)\n",
        "                i = 1\n",
        "                while os.path.exists(os.path.join(dest_folder, f\"{base}_{i}{ext}\")):\n",
        "                    i += 1\n",
        "                dest_path = os.path.join(dest_folder, f\"{base}_{i}{ext}\")\n",
        "\n",
        "            shutil.move(src_path, dest_path)\n",
        "\n",
        "print(\"Finished merging into 'class-wise dataset'\")\n"
      ],
      "metadata": {
        "id": "ALTtw5DWkhtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find individual class sizes\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/Ocular Disease Dataset 1/Class-wise Data\"\n",
        "\n",
        "class_counts = {}\n",
        "for cls in os.listdir(data_dir):\n",
        "    cls_path = os.path.join(data_dir, cls)\n",
        "    if os.path.isdir(cls_path):\n",
        "        class_counts[cls] = len(os.listdir(cls_path))\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "\n",
        "max_count = max(class_counts.values())\n",
        "print(\"Majority class size:\", max_count)\n"
      ],
      "metadata": {
        "id": "yyTRFH6bk1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finds image number from image name\n",
        "def number_parser(num_string):\n",
        "  number = ''\n",
        "  for i in num_string:\n",
        "    if i!='_':\n",
        "      number = number+i\n",
        "    else:\n",
        "      break\n",
        "  return number"
      ],
      "metadata": {
        "id": "Etjqvlrcq_Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subject-level split\n",
        "\n",
        "import random\n",
        "random.seed(90)\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Ocular Disease Dataset 1/full_df.csv\")\n",
        "list_main = []\n",
        "list_training = []\n",
        "for i in range(6392):\n",
        "  list_main.append([data['filename'][i], data['target'][i]])\n",
        "random.shuffle(list_main)\n",
        "for k in range(20):\n",
        "  print(list_main[k])\n",
        "print('          ')\n",
        "print('          ')\n",
        "for i in range(2237):\n",
        "  a = list_main[i]\n",
        "  a_name = a[0]\n",
        "  a_num = number_parser(a_name)\n",
        "  if \"right\" in a_name:\n",
        "    b_name = a_num+\"_left.jpg\"\n",
        "    for k, sublist in enumerate(list_main):\n",
        "      if b_name in sublist:\n",
        "        b = list_main[k]\n",
        "        break\n",
        "  list_training.append(a)\n",
        "  list_training.append(b)\n",
        "for e in range(20):\n",
        "  print(list_training[e])"
      ],
      "metadata": {
        "id": "kl1JGMCpk6w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store pairs in dictionary and split\n",
        "import random\n",
        "subject_map = {}\n",
        "for i in range(len(data)):\n",
        "    fname = data['filename'][i]\n",
        "    target = data['target'][i]\n",
        "    subj_id = fname.split(\"_\")[0]   # \"1485\"\n",
        "    subject_map.setdefault(subj_id, []).append([fname, target])\n",
        "subjects = list(subject_map.values())\n",
        "random.seed(90)\n",
        "random.shuffle(subjects)\n",
        "\n",
        "# Split 70/30\n",
        "n_train = int(len(subjects) * 0.7)\n",
        "train_subjects = subjects[:n_train]\n",
        "test_subjects = subjects[n_train:]"
      ],
      "metadata": {
        "id": "1WecF5lfly03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten into training, testing list\n",
        "list_training = [sample for subj in train_subjects for sample in subj]\n",
        "list_testing  = [sample for subj in test_subjects for sample in subj]\n",
        "\n",
        "print(\"Training images:\", len(list_training))\n",
        "print(\"Testing images:\", len(list_testing))\n"
      ],
      "metadata": {
        "id": "fUMEChxKm6FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Move each image into training folder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 1/preprocessed_images\"\n",
        "for value in list_training:\n",
        "  name = value[0]\n",
        "  disease = ''\n",
        "  if value[1][1] == '1':\n",
        "    disease = \"Normal\"\n",
        "  elif value[1][4] == '1':\n",
        "    disease = \"Diabetes\"\n",
        "  elif value[1][7] == '1':\n",
        "    disease = \"Glaucoma\"\n",
        "  elif value[1][10] == '1':\n",
        "    disease = \"Cataract\"\n",
        "  elif value[1][13] == '1':\n",
        "    disease = \"ARMD\"\n",
        "  elif value[1][16] == '1':\n",
        "    disease = \"Hypertension\"\n",
        "  elif value[1][19] == '1':\n",
        "    disease = \"Myopia\"\n",
        "  elif value[1][22] == '1':\n",
        "    disease = \"Other\"\n",
        "  dst_folder = f\"/content/drive/My Drive/Ocular Disease Dataset 1/Training Data/{disease}\"\n",
        "  src_path = os.path.join(src_folder, name)\n",
        "  dst_path = os.path.join(dst_folder, name)\n",
        "  shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "e0JYbBRPnH4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Moves each image into testing folder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 1/preprocessed_images\"\n",
        "for value in list_testing:\n",
        "  name = value[0]\n",
        "  disease = ''\n",
        "  if value[1][1] == '1':\n",
        "    disease = \"Normal\"\n",
        "  elif value[1][4] == '1':\n",
        "    disease = \"Diabetes\"\n",
        "  elif value[1][7] == '1':\n",
        "    disease = \"Glaucoma\"\n",
        "  elif value[1][10] == '1':\n",
        "    disease = \"Cataract\"\n",
        "  elif value[1][13] == '1':\n",
        "    disease = \"ARMD\"\n",
        "  elif value[1][16] == '1':\n",
        "    disease = \"Hypertension\"\n",
        "  elif value[1][19] == '1':\n",
        "    disease = \"Myopia\"\n",
        "  elif value[1][22] == '1':\n",
        "    disease = \"Other\"\n",
        "  dst_folder = f\"/content/drive/My Drive/Ocular Disease Dataset 1/Testing Data/{disease}\"\n",
        "  src_path = os.path.join(src_folder, name)\n",
        "  dst_path = os.path.join(dst_folder, name)\n",
        "  shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "4O4BaDCQnSKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits into 80% training, 20% validation\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "train_base = \"/content/drive/My Drive/Ocular Disease Dataset 1/Training Data\"\n",
        "val_base   = \"/content/drive/My Drive/Ocular Disease Dataset 1/Validation Data\"\n",
        "\n",
        "# Make sure validation subfolders exist\n",
        "classes = os.listdir(train_base)  # expects 8 subfolders already created\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(val_base, cls), exist_ok=True)\n",
        "split_ratio = 0.2  # 20% validation\n",
        "random.seed(90)    # reproducibility\n",
        "\n",
        "for cls in classes:\n",
        "    train_cls_folder = os.path.join(train_base, cls)\n",
        "    val_cls_folder   = os.path.join(val_base, cls)\n",
        "\n",
        "    images = os.listdir(train_cls_folder)\n",
        "\n",
        "    random.shuffle(images)\n",
        "    n_val = int(len(images) * split_ratio)\n",
        "    val_images = images[:n_val]\n",
        "\n",
        "    # move each selected image into validation\n",
        "    for img in val_images:\n",
        "        src = os.path.join(train_cls_folder, img)\n",
        "        dst = os.path.join(val_cls_folder, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    print(f\"{cls}: moved {n_val} → validation, kept {len(images) - n_val} in training\")\n"
      ],
      "metadata": {
        "id": "5TrEarkcnac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Oversampling and Data Augmentation for Training Dataset\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "\n",
        "# Paths\n",
        "src_base = \"/content/drive/My Drive/Ocular Disease Dataset 1/Training Data\"\n",
        "out_base = \"/content/drive/My Drive/Ocular Disease Dataset 1/Balanced_Training_Data\"\n",
        "\n",
        "classes = [\"ARMD\",\"Cataract\",\"Diabetes\",\"Glaucoma\",\"Hypertension\",\"Myopia\",\"Normal\",\"Other\"]\n",
        "\n",
        "# augmentation generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# target count = Normal class count\n",
        "target_count = 1617\n",
        "\n",
        "for cls in classes:\n",
        "    src_folder = os.path.join(src_base, cls)\n",
        "    dst_folder = os.path.join(out_base, cls)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    images = os.listdir(src_folder)\n",
        "    count = len(images)\n",
        "\n",
        "    print(f\"{cls}: {count} images → target {target_count}\")\n",
        "\n",
        "    # Step 1. copy originals\n",
        "    for img in images:\n",
        "        src_path = os.path.join(src_folder, img)\n",
        "        shutil.copy(src_path, os.path.join(dst_folder, img))\n",
        "\n",
        "    # Step 2. oversample + augment minority classes\n",
        "    i = 0\n",
        "    while count < target_count:\n",
        "        img_name = images[i % len(images)]  # cycle through originals\n",
        "        img_path = os.path.join(src_folder, img_name)\n",
        "\n",
        "        # load image\n",
        "        img = load_img(img_path)\n",
        "        x = img_to_array(img)\n",
        "        x = x.reshape((1,) + x.shape)\n",
        "\n",
        "        # generate one augmented image\n",
        "        aug_iter = datagen.flow(x, batch_size=1)\n",
        "        aug_img = next(aug_iter)[0].astype(\"uint8\")\n",
        "\n",
        "        # save with new name\n",
        "        new_name = f\"{os.path.splitext(img_name)[0]}_aug_{count}.jpg\"\n",
        "        array_to_img(aug_img).save(os.path.join(dst_folder, new_name))\n",
        "\n",
        "        count += 1\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Balanced {cls}: {len(os.listdir(dst_folder))} images\")"
      ],
      "metadata": {
        "id": "IY3qn8FCnoc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 2 - Preprocessing**"
      ],
      "metadata": {
        "id": "ODcRrfDjrS-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle and separate each dataset into training and testing data\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "random.seed(90)\n",
        "source_1 = os.listdir('/content/drive/My Drive/Ocular Disease Dataset 2/dataset/normal')\n",
        "source_2 = os.listdir('/content/drive/My Drive/Ocular Disease Dataset 2/dataset/diabetic_retinopathy')\n",
        "source_3 = os.listdir('/content/drive/My Drive/Ocular Disease Dataset 2/dataset/glaucoma')\n",
        "source_4 = os.listdir('/content/drive/My Drive/Ocular Disease Dataset 2/dataset/cataract')\n",
        "random.shuffle(source_1)\n",
        "random.shuffle(source_2)\n",
        "random.shuffle(source_3)\n",
        "random.shuffle(source_4)\n",
        "training_data_1 = []\n",
        "testing_data_1 = []\n",
        "training_data_2 = []\n",
        "testing_data_2 = []\n",
        "training_data_3 = []\n",
        "testing_data_3 = []\n",
        "training_data_4 = []\n",
        "testing_data_4 = []\n",
        "\n",
        "for i in range(int(0.7*len(source_1))):\n",
        "  training_data_1.append(source_1[i])\n",
        "for j in range(int(0.7*len(source_1)), len(source_1)):\n",
        "  testing_data_1.append(source_1[j])\n",
        "\n",
        "for i in range(int(0.7*len(source_2))):\n",
        "  training_data_2.append(source_2[i])\n",
        "for j in range(int(0.7*len(source_2)), len(source_2)):\n",
        "  testing_data_2.append(source_2[j])\n",
        "\n",
        "for i in range(int(0.7*len(source_3))):\n",
        "  training_data_3.append(source_3[i])\n",
        "for j in range(int(0.7*len(source_3)), len(source_3)):\n",
        "  testing_data_3.append(source_3[j])\n",
        "\n",
        "for i in range(int(0.7*len(source_4))):\n",
        "  training_data_4.append(source_4[i])\n",
        "for j in range(int(0.7*len(source_4)), len(source_4)):\n",
        "  testing_data_4.append(source_4[j])\n",
        "\n"
      ],
      "metadata": {
        "id": "lkJpfAmBpnTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset/normal\"\n",
        "dst_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data/Normal\"\n",
        "for i in training_data_1:\n",
        "  src_path = os.path.join(src_folder, i)\n",
        "  dst_path = os.path.join(dst_folder, i)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Normal Training\")\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset/normal\"\n",
        "dst_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/Testing Data/Normal\"\n",
        "for i in testing_data_1:\n",
        "  src_path = os.path.join(src_folder, i)\n",
        "  dst_path = os.path.join(dst_folder, i)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Normal Testing\")\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset/diabetic_retinopathy\"\n",
        "dst_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data/Diabetes\"\n",
        "for i in training_data_2:\n",
        "  src_path = os.path.join(src_folder, i)\n",
        "  dst_path = os.path.join(dst_folder, i)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Diabetes Training\")\n",
        "\n",
        "dst_folder_new = \"/content/drive/My Drive/Ocular Disease Dataset 2/Testing Data/Diabetes\"\n",
        "for j in testing_data_2:\n",
        "  src_path = os.path.join(src_folder, j)\n",
        "  dst_path = os.path.join(dst_folder_new, j)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Diabetes Testing\")\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset/glaucoma\"\n",
        "dst_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data/Glaucoma\"\n",
        "for i in training_data_3:\n",
        "  src_path = os.path.join(src_folder, i)\n",
        "  dst_path = os.path.join(dst_folder, i)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Glaucoma Training\")\n",
        "\n",
        "dst_folder_new = \"/content/drive/My Drive/Ocular Disease Dataset 2/Testing Data/Glaucoma\"\n",
        "for j in testing_data_3:\n",
        "  src_path = os.path.join(src_folder, j)\n",
        "  dst_path = os.path.join(dst_folder_new, j)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Glaucoma Testing\")\n",
        "\n",
        "src_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset/cataract\"\n",
        "dst_folder = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data/Cataract\"\n",
        "for i in training_data_4:\n",
        "  src_path = os.path.join(src_folder, i)\n",
        "  dst_path = os.path.join(dst_folder, i)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Cataract Training\")\n",
        "\n",
        "dst_folder_new = \"/content/drive/My Drive/Ocular Disease Dataset 2/Testing Data/Cataract\"\n",
        "for j in testing_data_4:\n",
        "  src_path = os.path.join(src_folder, j)\n",
        "  dst_path = os.path.join(dst_folder_new, j)\n",
        "  shutil.copy(src_path, dst_path)\n",
        "print(\"Done With Cataract Testing\")"
      ],
      "metadata": {
        "id": "0gBRNI8jqHx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create and separate validation data\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "train_base = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data\"\n",
        "val_base   = \"/content/drive/My Drive/Ocular Disease Dataset 2/Validation Data\"\n",
        "\n",
        "# Make sure validation subfolders exist\n",
        "classes = os.listdir(train_base)  # expects 8 subfolders already created\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(val_base, cls), exist_ok=True)\n",
        "split_ratio = 0.2  # 20% validation\n",
        "random.seed(90)    # reproducibility\n",
        "\n",
        "for cls in classes:\n",
        "    train_cls_folder = os.path.join(train_base, cls)\n",
        "    val_cls_folder   = os.path.join(val_base, cls)\n",
        "\n",
        "    # get all images in this class\n",
        "    images = os.listdir(train_cls_folder)\n",
        "\n",
        "    # shuffle and split\n",
        "    random.shuffle(images)\n",
        "    n_val = int(len(images) * split_ratio)\n",
        "    val_images = images[:n_val]\n",
        "\n",
        "    # move each selected image into validation\n",
        "    for img in val_images:\n",
        "        src = os.path.join(train_cls_folder, img)\n",
        "        dst = os.path.join(val_cls_folder, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    print(f\"{cls}: moved {n_val} → validation, kept {len(images) - n_val} in training\")"
      ],
      "metadata": {
        "id": "9soy-baIqRv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Balance training data\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "\n",
        "# Paths\n",
        "train_base = \"/content/drive/My Drive/Ocular Disease Dataset 2/Training Data\"\n",
        "balanced_train = \"/content/drive/My Drive/Ocular Disease Dataset 2/Balanced Training Data\"\n",
        "os.makedirs(balanced_train, exist_ok=True)\n",
        "\n",
        "# Augmentation settings\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Step 1: Count images per class\n",
        "counts = {cls: len(os.listdir(os.path.join(train_base, cls))) for cls in os.listdir(train_base)}\n",
        "max_count = max(counts.values())\n",
        "print(\"Target count per class:\", max_count)\n",
        "\n",
        "# Step 2: Balance each class\n",
        "for cls in os.listdir(train_base):\n",
        "    src_folder = os.path.join(train_base, cls)\n",
        "    dst_folder = os.path.join(balanced_train, cls)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    images = os.listdir(src_folder)\n",
        "    count = len(images)\n",
        "\n",
        "    print(f\"{cls}: {count} images → balancing to {max_count}\")\n",
        "\n",
        "    # Copy originals\n",
        "    for img in images:\n",
        "        shutil.copy(os.path.join(src_folder, img), os.path.join(dst_folder, img))\n",
        "\n",
        "    # Oversample with augmentation\n",
        "    i = 0\n",
        "    while count < max_count:\n",
        "        img_name = images[i % len(images)]  # cycle through originals\n",
        "        img_path = os.path.join(src_folder, img_name)\n",
        "\n",
        "        # Load image\n",
        "        img = load_img(img_path)\n",
        "        x = img_to_array(img)\n",
        "        x = x.reshape((1,) + x.shape)\n",
        "\n",
        "        # Generate augmented image\n",
        "        aug_iter = datagen.flow(x, batch_size=1)\n",
        "        aug_img = next(aug_iter)[0].astype(\"uint8\")\n",
        "\n",
        "        # Save augmented image\n",
        "        new_name = f\"{os.path.splitext(img_name)[0]}_aug_{count}.jpg\"\n",
        "        array_to_img(aug_img).save(os.path.join(dst_folder, new_name))\n",
        "\n",
        "        count += 1\n",
        "        i += 1\n",
        "\n",
        "    print(f\"{cls} balanced: {len(os.listdir(dst_folder))} images\")"
      ],
      "metadata": {
        "id": "fO_UyW9Xqeo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 1 - Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "7NiBxwA3raov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121, VGG16"
      ],
      "metadata": {
        "id": "Anko7eAGqqXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 1/Balanced_Training_Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))\n",
        "\n",
        "val_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 1/Validation Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))\n",
        "\n",
        "test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 1/Testing Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))"
      ],
      "metadata": {
        "id": "xt5KDOHHs0vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Speed up training\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "YKYQIv08s2BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 - Model Training"
      ],
      "metadata": {
        "id": "jeI4wDJfs9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='ResNet50', input_shape=(512, 512, 3), num_classes=8):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "qid2D84Ks65u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='ResNet50', input_shape=(512, 512, 3), num_classes=8)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)\n",
        "\n",
        "#IMPORTANT: After training this model, skip the next two sections (DenseNet121 and VGG16 Model training) and directly begin with \"Evaluations and Graphs\" section"
      ],
      "metadata": {
        "id": "3Zt65yR7t2SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet121 - Model Training"
      ],
      "metadata": {
        "id": "411syYu_twUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='DenseNet121', input_shape=(512, 512, 3), num_classes=8):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "T21PDvC9uFLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='DenseNet121', input_shape=(512, 512, 3), num_classes=8)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)"
      ],
      "metadata": {
        "id": "tCBqzLpXuHen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16 - Model Training"
      ],
      "metadata": {
        "id": "fnTd8m6EuJ-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='VGG16', input_shape=(512, 512, 3), num_classes=8):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Q0Gl3wuZuM08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='VGG16', input_shape=(512, 512, 3), num_classes=8)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)"
      ],
      "metadata": {
        "id": "KMhAXO_YuPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation and Graphs"
      ],
      "metadata": {
        "id": "Eapv3gSLuofv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation with Accuracy, Precision and Recall\n",
        "loss, accuracy, precision, recall = model.evaluate(test_ds)\n",
        "#print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "id": "9ZyHgHJiuW_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UazebQaJtCln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')"
      ],
      "metadata": {
        "id": "3DXOhCLvtEMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))      # Convert one-hot to class index\n",
        "    y_pred.extend(np.argmax(preds, axis=1))                # Predicted class index\n",
        "    print(labels.shape)\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)"
      ],
      "metadata": {
        "id": "-sdTFaMMtGAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1','Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "Za4MVopatHir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1','Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7'], yticklabels=['Class 0', 'Class 1','Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vHpLY28mtJXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class-wise accuracy, ROC and PR Curves, ROC-AUC and PR-AUC Values, CI\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score,\n",
        "                             roc_curve, precision_recall_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "#Ensure test dataset has deterministic order\n",
        "BATCH = 32\n",
        "test_eval = test_ds.unbatch().batch(BATCH)  # preserves order\n",
        "test_eval = test_eval.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#Materialize arrays ONCE to guarantee alignment\n",
        "X_list, Y_list = [], []\n",
        "for x,y in test_eval:\n",
        "    X_list.append(x.numpy())\n",
        "    Y_list.append(y.numpy())\n",
        "X = np.concatenate(X_list, axis=0)             # [N, H, W, C]\n",
        "y_true_onehot = np.concatenate(Y_list, axis=0) # [N, 8]\n",
        "y_true = y_true_onehot.argmax(axis=1)          # [N]\n",
        "\n",
        "#Predict probabilities aligned with y_true\n",
        "y_prob = model.predict(X, batch_size=BATCH, verbose=0)  # [N, 8]\n",
        "y_pred = y_prob.argmax(axis=1)\n",
        "\n",
        "# Sanity checks\n",
        "assert y_prob.shape[0] == y_true.shape[0]\n",
        "assert np.allclose(y_prob.sum(axis=1), 1, atol=1e-6)  # softmax\n",
        "\n",
        "#3) Micro accuracy + Wilson CI\n",
        "def wilson_ci(k, n, confidence=0.95):\n",
        "    if n == 0: return (0.0, 0.0)\n",
        "    z = norm.ppf(0.5 + confidence/2.0)\n",
        "    phat = k/n\n",
        "    denom = 1 + (z**2)/n\n",
        "    center = (phat + (z**2)/(2*n)) / denom\n",
        "    half = (z * ((phat*(1-phat)/n + (z**2)/(4*n**2))**0.5)) / denom\n",
        "    return center - half, center + half\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=np.arange(y_true_onehot.shape[1]))\n",
        "diag = np.diag(cm)\n",
        "total_correct = diag.sum()\n",
        "total = cm.sum()\n",
        "\n",
        "micro_acc = total_correct / total\n",
        "lo_acc, hi_acc = wilson_ci(int(total_correct), int(total))\n",
        "print(f\"Micro accuracy = {micro_acc:.4f} (95% CI: {lo_acc:.4f}–{hi_acc:.4f})\")\n",
        "\n",
        "#Compare with Keras categorical accuracy to prove consistency\n",
        "cat_acc = CategoricalAccuracy()\n",
        "cat_acc.update_state(y_true_onehot, y_prob)\n",
        "print(f\"Keras CategoricalAccuracy = {cat_acc.result().numpy():.4f}\")\n",
        "\n",
        "#Per-class “accuracy” (row recall) + Wilson CI\n",
        "row_sums = cm.sum(axis=1)\n",
        "per_class_acc = diag / np.maximum(row_sums, 1)  # avoid /0\n",
        "\n",
        "print(\"\\nPer-class accuracy (i.e., recall/sensitivity):\")\n",
        "for i in range(len(per_class_acc)):\n",
        "    k_i, n_i = int(diag[i]), int(row_sums[i])\n",
        "    lo_i, hi_i = wilson_ci(k_i, n_i) if n_i > 0 else (0.0, 0.0)\n",
        "    print(f\"Class {i}: Acc={per_class_acc[i]:.4f} (95% CI: {lo_i:.4f}–{hi_i:.4f})\")\n",
        "\n",
        "#ROC-AUC & PR-AUC (per-class + micro & macro)\n",
        "n_classes = y_true_onehot.shape[1]\n",
        "\n",
        "#Per-class curves\n",
        "plt.figure()\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_onehot[:, i], y_prob[:, i])\n",
        "    plt.plot(fpr, tpr, label=f\"Class {i}\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multiclass ROC\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for i in range(n_classes):\n",
        "    prec, rec, _ = precision_recall_curve(y_true_onehot[:, i], y_prob[:, i])\n",
        "    plt.plot(rec, prec, label=f\"Class {i}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Multiclass PR\"); plt.legend(); plt.show()\n",
        "\n",
        "#AUC\n",
        "roc_auc_per_class = [roc_auc_score(y_true_onehot[:, i], y_prob[:, i]) for i in range(n_classes)]\n",
        "pr_auc_per_class  = [average_precision_score(y_true_onehot[:, i], y_prob[:, i]) for i in range(n_classes)]\n",
        "\n",
        "roc_auc_micro = roc_auc_score(y_true_onehot, y_prob, average=\"micro\", multi_class=\"ovr\")\n",
        "roc_auc_macro = roc_auc_score(y_true_onehot, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
        "pr_auc_micro  = average_precision_score(y_true_onehot, y_prob, average=\"micro\")\n",
        "pr_auc_macro  = average_precision_score(y_true_onehot, y_prob, average=\"macro\")\n",
        "\n",
        "print(\"\\nROC-AUC per class:\", [f\"{v:.3f}\" for v in roc_auc_per_class])\n",
        "print(\"PR-AUC  per class:\", [f\"{v:.3f}\" for v in pr_auc_per_class])\n",
        "print(f\"Micro ROC-AUC = {roc_auc_micro:.3f} | Macro ROC-AUC = {roc_auc_macro:.3f}\")\n",
        "print(f\"Micro PR-AUC  = {pr_auc_micro:.3f}  | Macro PR-AUC  = {pr_auc_macro:.3f}\")\n",
        "\n",
        "#Bootstrap CIs for micro AUC\n",
        "def bootstrap_ci(metric_fn, y_true_bin, y_score, n_boot=2000, confidence=0.95, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = y_true_bin.shape[0]\n",
        "    stats = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        stats.append(metric_fn(y_true_bin[idx], y_score[idx]))\n",
        "    lo = np.percentile(stats, (1-confidence)/2*100)\n",
        "    hi = np.percentile(stats, (1+confidence)/2*100)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "roc_micro_ci = bootstrap_ci(lambda yt, yp: roc_auc_score(yt, yp, average=\"micro\", multi_class=\"ovr\"),\n",
        "                            y_true_onehot, y_prob)\n",
        "pr_micro_ci  = bootstrap_ci(lambda yt, yp: average_precision_score(yt, yp, average=\"micro\"),\n",
        "                            y_true_onehot, y_prob)\n",
        "\n",
        "print(f\"\\nMicro ROC-AUC 95% CI: {roc_micro_ci[0]:.3f}–{roc_micro_ci[1]:.3f}\")\n",
        "print(f\"Micro PR-AUC  95% CI: {pr_micro_ci[0]:.3f}–{pr_micro_ci[1]:.3f}\")"
      ],
      "metadata": {
        "id": "VZQQs42wtK4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 2 - Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "HNJ4F0Nuv6YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9upZfLxYwA90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121, VGG16"
      ],
      "metadata": {
        "id": "9wTmnhhNwCY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 2/Balanced Training Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))\n",
        "\n",
        "val_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 2/Validation Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))\n",
        "\n",
        "test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/drive/My Drive/Ocular Disease Dataset 2/Testing Data',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(512, 512))"
      ],
      "metadata": {
        "id": "OsKlQAcIwJxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Speed up training\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Pbb6dBaWwLDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 - Model Training"
      ],
      "metadata": {
        "id": "hm2OqGGmwPLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='ResNet50', input_shape=(512, 512, 3), num_classes=4):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "sWfhY6AswMi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='ResNet50', input_shape=(512, 512, 3), num_classes=4)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)\n",
        "\n",
        "#IMPORTANT: After training this model, skip the next two sections (DenseNet121 and VGG16 Model training) and directly begin with \"Evaluations and Graphs\" section"
      ],
      "metadata": {
        "id": "kxKJg0cAwOFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet121 - Model Training"
      ],
      "metadata": {
        "id": "D5UwOQ-1xqdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='DenseNet121', input_shape=(512, 512, 3), num_classes=4):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "VwTT0kwCxsgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='DenseNet121', input_shape=(512, 512, 3), num_classes=4)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)"
      ],
      "metadata": {
        "id": "Sfx7d6kVxzLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16 - Model Training"
      ],
      "metadata": {
        "id": "dXBjiENjx1HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_name='VGG16', input_shape=(512, 512, 3), num_classes=4):\n",
        "    # Choose backbone\n",
        "    if base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    elif base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from 'ResNet50', 'DenseNet121', or 'VGG16'\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "d1mmMSV-x-7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(base_model_name='VGG16', input_shape=(512, 512, 3), num_classes=4)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='pro_architecture_best_model_by_precision.weights.h5',\n",
        "    monitor='val_precision',       # ← track validation precision\n",
        "    mode='max',                    # because higher precision is better\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=30)"
      ],
      "metadata": {
        "id": "zpSBfRIWyB-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation and Graphs"
      ],
      "metadata": {
        "id": "qq1DJrUQyG0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy, precision, recall = model.evaluate(test_ds)\n",
        "#print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")"
      ],
      "metadata": {
        "id": "YTnyPgiMyJyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZSvsoTvVyKc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')"
      ],
      "metadata": {
        "id": "8vARgIosyLlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))      # Convert one-hot to class index\n",
        "    y_pred.extend(np.argmax(preds, axis=1))                # Predicted class index\n",
        "    print(labels.shape)\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)"
      ],
      "metadata": {
        "id": "c23anymByMxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1','Class 2', 'Class 3']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "1qXRxrx6yN_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1','Class 2', 'Class 3'], yticklabels=['Class 0', 'Class 1','Class 2', 'Class 3'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKiSQwaiyPTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class-wise accuracy, ROC and PR Curves, ROC-AUC and PR-AUC Values, CI\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score,\n",
        "                             roc_curve, precision_recall_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "#Ensure test dataset has deterministic order\n",
        "BATCH = 32\n",
        "test_eval = test_ds.unbatch().batch(BATCH)  # preserves order\n",
        "test_eval = test_eval.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#Materialize arrays ONCE to guarantee alignment\n",
        "X_list, Y_list = [], []\n",
        "for x,y in test_eval:\n",
        "    X_list.append(x.numpy())\n",
        "    Y_list.append(y.numpy())\n",
        "X = np.concatenate(X_list, axis=0)             # [N, H, W, C]\n",
        "y_true_onehot = np.concatenate(Y_list, axis=0) # [N, 8]\n",
        "y_true = y_true_onehot.argmax(axis=1)          # [N]\n",
        "\n",
        "#Predict probabilities aligned with y_true\n",
        "y_prob = model.predict(X, batch_size=BATCH, verbose=0)  # [N, 8]\n",
        "y_pred = y_prob.argmax(axis=1)\n",
        "\n",
        "# Sanity checks\n",
        "assert y_prob.shape[0] == y_true.shape[0]\n",
        "assert np.allclose(y_prob.sum(axis=1), 1, atol=1e-6)  # softmax\n",
        "\n",
        "#3) Micro accuracy + Wilson CI\n",
        "def wilson_ci(k, n, confidence=0.95):\n",
        "    if n == 0: return (0.0, 0.0)\n",
        "    z = norm.ppf(0.5 + confidence/2.0)\n",
        "    phat = k/n\n",
        "    denom = 1 + (z**2)/n\n",
        "    center = (phat + (z**2)/(2*n)) / denom\n",
        "    half = (z * ((phat*(1-phat)/n + (z**2)/(4*n**2))**0.5)) / denom\n",
        "    return center - half, center + half\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=np.arange(y_true_onehot.shape[1]))\n",
        "diag = np.diag(cm)\n",
        "total_correct = diag.sum()\n",
        "total = cm.sum()\n",
        "\n",
        "micro_acc = total_correct / total\n",
        "lo_acc, hi_acc = wilson_ci(int(total_correct), int(total))\n",
        "print(f\"Micro accuracy = {micro_acc:.4f} (95% CI: {lo_acc:.4f}–{hi_acc:.4f})\")\n",
        "\n",
        "#Compare with Keras categorical accuracy to prove consistency\n",
        "cat_acc = CategoricalAccuracy()\n",
        "cat_acc.update_state(y_true_onehot, y_prob)\n",
        "print(f\"Keras CategoricalAccuracy = {cat_acc.result().numpy():.4f}\")\n",
        "\n",
        "#Per-class “accuracy” (row recall) + Wilson CI\n",
        "row_sums = cm.sum(axis=1)\n",
        "per_class_acc = diag / np.maximum(row_sums, 1)  # avoid /0\n",
        "\n",
        "print(\"\\nPer-class accuracy (i.e., recall/sensitivity):\")\n",
        "for i in range(len(per_class_acc)):\n",
        "    k_i, n_i = int(diag[i]), int(row_sums[i])\n",
        "    lo_i, hi_i = wilson_ci(k_i, n_i) if n_i > 0 else (0.0, 0.0)\n",
        "    print(f\"Class {i}: Acc={per_class_acc[i]:.4f} (95% CI: {lo_i:.4f}–{hi_i:.4f})\")\n",
        "\n",
        "#ROC-AUC & PR-AUC (per-class + micro & macro)\n",
        "n_classes = y_true_onehot.shape[1]\n",
        "\n",
        "#Per-class curves\n",
        "plt.figure()\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_onehot[:, i], y_prob[:, i])\n",
        "    plt.plot(fpr, tpr, label=f\"Class {i}\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multiclass ROC\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for i in range(n_classes):\n",
        "    prec, rec, _ = precision_recall_curve(y_true_onehot[:, i], y_prob[:, i])\n",
        "    plt.plot(rec, prec, label=f\"Class {i}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Multiclass PR\"); plt.legend(); plt.show()\n",
        "\n",
        "#AUC\n",
        "roc_auc_per_class = [roc_auc_score(y_true_onehot[:, i], y_prob[:, i]) for i in range(n_classes)]\n",
        "pr_auc_per_class  = [average_precision_score(y_true_onehot[:, i], y_prob[:, i]) for i in range(n_classes)]\n",
        "\n",
        "roc_auc_micro = roc_auc_score(y_true_onehot, y_prob, average=\"micro\", multi_class=\"ovr\")\n",
        "roc_auc_macro = roc_auc_score(y_true_onehot, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
        "pr_auc_micro  = average_precision_score(y_true_onehot, y_prob, average=\"micro\")\n",
        "pr_auc_macro  = average_precision_score(y_true_onehot, y_prob, average=\"macro\")\n",
        "\n",
        "print(\"\\nROC-AUC per class:\", [f\"{v:.3f}\" for v in roc_auc_per_class])\n",
        "print(\"PR-AUC  per class:\", [f\"{v:.3f}\" for v in pr_auc_per_class])\n",
        "print(f\"Micro ROC-AUC = {roc_auc_micro:.3f} | Macro ROC-AUC = {roc_auc_macro:.3f}\")\n",
        "print(f\"Micro PR-AUC  = {pr_auc_micro:.3f}  | Macro PR-AUC  = {pr_auc_macro:.3f}\")\n",
        "\n",
        "#Bootstrap CIs for micro AUC\n",
        "def bootstrap_ci(metric_fn, y_true_bin, y_score, n_boot=2000, confidence=0.95, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = y_true_bin.shape[0]\n",
        "    stats = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        stats.append(metric_fn(y_true_bin[idx], y_score[idx]))\n",
        "    lo = np.percentile(stats, (1-confidence)/2*100)\n",
        "    hi = np.percentile(stats, (1+confidence)/2*100)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "roc_micro_ci = bootstrap_ci(lambda yt, yp: roc_auc_score(yt, yp, average=\"micro\", multi_class=\"ovr\"),\n",
        "                            y_true_onehot, y_prob)\n",
        "pr_micro_ci  = bootstrap_ci(lambda yt, yp: average_precision_score(yt, yp, average=\"micro\"),\n",
        "                            y_true_onehot, y_prob)\n",
        "\n",
        "print(f\"\\nMicro ROC-AUC 95% CI: {roc_micro_ci[0]:.3f}–{roc_micro_ci[1]:.3f}\")\n",
        "print(f\"Micro PR-AUC  95% CI: {pr_micro_ci[0]:.3f}–{pr_micro_ci[1]:.3f}\")"
      ],
      "metadata": {
        "id": "RmIBm462yQwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning"
      ],
      "metadata": {
        "id": "fZlNfeBwzIKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.stats import mode\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch"
      ],
      "metadata": {
        "id": "29U9a3b-zNKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ws-gHXpb1O8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 1**"
      ],
      "metadata": {
        "id": "ptzB9FxW1Sjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/Ocular Disease Dataset 1/Unsupervised Test\",\n",
        "    image_size=(224, 224),   # Resize all images\n",
        "    batch_size=512,         # Get individual images\n",
        "    shuffle=False            # Keep order for label tracking\n",
        ")"
      ],
      "metadata": {
        "id": "t6DP7YJV1QEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing using .map()\n",
        "dataset = dataset.map(lambda x, y: (preprocess_input(x), y))\n",
        "\n",
        "# Load pretrained model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Extract features in batches\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for batch_images, batch_labels in dataset:\n",
        "    batch_features = base_model.predict(batch_images)\n",
        "    features.append(batch_features)\n",
        "    labels.append(batch_labels.numpy())\n",
        "\n",
        "features = np.concatenate(features)\n",
        "labels = np.concatenate(labels)"
      ],
      "metadata": {
        "id": "BhyTwyKM1Rkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Right now using K-Means, can try DBSCAN or Agglomerative Clustering\n",
        "n_clusters = len(np.unique(labels))  # Number of actual classes\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "pred_clusters = kmeans.fit_predict(features)\n",
        "# Now you have a feature matrix (features) for your images."
      ],
      "metadata": {
        "id": "1PR0sSUu1Vxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ari = adjusted_rand_score(labels, pred_clusters)\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "\n",
        "nmi = normalized_mutual_info_score(labels, pred_clusters)\n",
        "print(f\"NMI: {nmi:.4f}\")"
      ],
      "metadata": {
        "id": "viYVVtHu1XAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_2d = TSNE(n_components=2, random_state=42).fit_transform(features)"
      ],
      "metadata": {
        "id": "9nNX-bBQ1YO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure labels and clusters are numpy arrays\n",
        "labels = np.array(labels)\n",
        "pred_clusters = np.array(pred_clusters)\n",
        "\n",
        "# Step 1: Define a consistent set of classes\n",
        "all_classes = np.unique(np.concatenate([labels, pred_clusters]))\n",
        "class_to_index = {cls: i for i, cls in enumerate(sorted(all_classes))}\n",
        "\n",
        "# Step 2: Choose a colormap with enough colors\n",
        "cmap = plt.get_cmap('tab10')  # Or use 'tab20' for more classes\n",
        "num_colors = len(all_classes)\n",
        "colors = [cmap(i % cmap.N) for i in range(num_colors)]  # wrap around if too many\n",
        "\n",
        "# Step 3: Build class-to-color mapping\n",
        "color_dict = {cls: colors[i] for cls, i in class_to_index.items()}\n",
        "\n",
        "# Step 4: Map each label to a color using the dict\n",
        "label_colors = [color_dict[l] for l in labels]\n",
        "cluster_colors = [color_dict[c] for c in pred_clusters]"
      ],
      "metadata": {
        "id": "qRUKxh_h1aXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_clusters_to_labels(true_labels, cluster_preds):\n",
        "    labels = np.zeros_like(cluster_preds)\n",
        "    for cluster_id in np.unique(cluster_preds):\n",
        "        mask = cluster_preds == cluster_id\n",
        "        labels[mask] = mode(true_labels[mask])[0]\n",
        "    return labels\n",
        "\n",
        "aligned_preds = map_clusters_to_labels(labels, pred_clusters)"
      ],
      "metadata": {
        "id": "BjgQnIso1bp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create color map based on unique true labels\n",
        "unique_labels = np.unique(labels)\n",
        "cmap = plt.get_cmap('tab10')\n",
        "color_list = [cmap(i) for i in range(len(unique_labels))]\n",
        "color_dict = {label: color_list[i] for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Map colors\n",
        "label_colors = [color_dict[l] for l in labels]\n",
        "aligned_cluster_colors = [color_dict[l] for l in aligned_preds]"
      ],
      "metadata": {
        "id": "JMNmH3sK1dLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class_names = dataset.class_names  # List of class names in order\n",
        "#class_name_dict = {i: name for i, name in enumerate(class_names)}\n",
        "class_name_dict = {\n",
        "    0: \"ARMD\",\n",
        "    1: \"Cataract\",\n",
        "    2: \"Diabetes\",\n",
        "    3: \"Glaucoma\",\n",
        "    4: \"Hypertension\",\n",
        "    5: \"Myopia\",\n",
        "    6: \"Normal\",\n",
        "    7: \"Other\"\n",
        "}\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# --- Plot 1: True Labels ---\n",
        "for label in unique_labels:\n",
        "    idx = labels == label\n",
        "    axes[0].scatter(X_2d[idx, 0], X_2d[idx, 1], c=[color_dict[label]],\n",
        "                    label=class_name_dict[label], s=20)\n",
        "axes[0].set_title('t-SNE Colored by True Labels')\n",
        "axes[0].set_xlabel('t-SNE 1')\n",
        "axes[0].set_ylabel('t-SNE 2')\n",
        "axes[0].legend(title=\"True Classes\", loc=\"best\", fontsize='small', markerscale=1.5)\n",
        "\n",
        "# --- Plot 2: Cluster Assignments (Aligned to True Labels) ---\n",
        "for label in unique_labels:\n",
        "    idx = aligned_preds == label\n",
        "    axes[1].scatter(X_2d[idx, 0], X_2d[idx, 1], c=[color_dict[label]],\n",
        "                    label=class_name_dict[label], s=20)\n",
        "axes[1].set_title('t-SNE Colored by K-Means Clusters')\n",
        "axes[1].set_xlabel('t-SNE 1')\n",
        "axes[1].set_ylabel('t-SNE 2')\n",
        "axes[1].legend(title=\"Cluster to Class\", loc=\"best\", fontsize='small', markerscale=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bS1mxL8z1faQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datset 2**"
      ],
      "metadata": {
        "id": "tdBLHRdm1pJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/Ocular Disease Dataset 2/dataset\",\n",
        "    image_size=(224, 224),   # Resize all images\n",
        "    batch_size=512,         # Get individual images\n",
        "    shuffle=False            # Keep order for label tracking\n",
        ")"
      ],
      "metadata": {
        "id": "DZHZCcBs1rdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing using .map()\n",
        "dataset = dataset.map(lambda x, y: (preprocess_input(x), y))\n",
        "\n",
        "# Load pretrained model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Extract features in batches\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for batch_images, batch_labels in dataset:\n",
        "    batch_features = base_model.predict(batch_images)\n",
        "    features.append(batch_features)\n",
        "    labels.append(batch_labels.numpy())\n",
        "\n",
        "features = np.concatenate(features)\n",
        "labels = np.concatenate(labels)"
      ],
      "metadata": {
        "id": "aYMPRcjA1u4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Right now using K-Means, can try DBSCAN or Agglomerative Clustering\n",
        "n_clusters = len(np.unique(labels))  # Number of actual classes\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "pred_clusters = kmeans.fit_predict(features)\n",
        "# Now you have a feature matrix (features) for your images."
      ],
      "metadata": {
        "id": "fkcQzwLp1wX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ari = adjusted_rand_score(labels, pred_clusters)\n",
        "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
        "\n",
        "nmi = normalized_mutual_info_score(labels, pred_clusters)\n",
        "print(f\"NMI: {nmi:.4f}\")"
      ],
      "metadata": {
        "id": "_i6BPcC51xop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_2d = TSNE(n_components=2, random_state=42).fit_transform(features)"
      ],
      "metadata": {
        "id": "hv0C4m5E14tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure labels and clusters are numpy arrays\n",
        "labels = np.array(labels)\n",
        "pred_clusters = np.array(pred_clusters)\n",
        "\n",
        "# Step 1: Define a consistent set of classes\n",
        "all_classes = np.unique(np.concatenate([labels, pred_clusters]))\n",
        "class_to_index = {cls: i for i, cls in enumerate(sorted(all_classes))}\n",
        "\n",
        "# Step 2: Choose a colormap with enough colors\n",
        "cmap = plt.get_cmap('tab10')  # Or use 'tab20' for more classes\n",
        "num_colors = len(all_classes)\n",
        "colors = [cmap(i % cmap.N) for i in range(num_colors)]  # wrap around if too many\n",
        "\n",
        "# Step 3: Build class-to-color mapping\n",
        "color_dict = {cls: colors[i] for cls, i in class_to_index.items()}\n",
        "\n",
        "# Step 4: Map each label to a color using the dict\n",
        "label_colors = [color_dict[l] for l in labels]\n",
        "cluster_colors = [color_dict[c] for c in pred_clusters]"
      ],
      "metadata": {
        "id": "BVtpqGub16A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_clusters_to_labels(true_labels, cluster_preds):\n",
        "    labels = np.zeros_like(cluster_preds)\n",
        "    for cluster_id in np.unique(cluster_preds):\n",
        "        mask = cluster_preds == cluster_id\n",
        "        labels[mask] = mode(true_labels[mask])[0]\n",
        "    return labels\n",
        "\n",
        "aligned_preds = map_clusters_to_labels(labels, pred_clusters)"
      ],
      "metadata": {
        "id": "8jyq0kq_18yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create color map based on unique true labels\n",
        "unique_labels = np.unique(labels)\n",
        "cmap = plt.get_cmap('tab10')\n",
        "color_list = [cmap(i) for i in range(len(unique_labels))]\n",
        "color_dict = {label: color_list[i] for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Map colors\n",
        "label_colors = [color_dict[l] for l in labels]\n",
        "aligned_cluster_colors = [color_dict[l] for l in aligned_preds]"
      ],
      "metadata": {
        "id": "l8LBirRl1-nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class_names = dataset.class_names  # List of class names in order\n",
        "#class_name_dict = {i: name for i, name in enumerate(class_names)}\n",
        "class_name_dict = {\n",
        "    0: \"Cataract\",\n",
        "    1: \"Diabetes\",\n",
        "    2: \"Glaucoma\",\n",
        "    3: \"Normal\",\n",
        "}\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# --- Plot 1: True Labels ---\n",
        "for label in unique_labels:\n",
        "    idx = labels == label\n",
        "    axes[0].scatter(X_2d[idx, 0], X_2d[idx, 1], c=[color_dict[label]],\n",
        "                    label=class_name_dict[label], s=20)\n",
        "axes[0].set_title('t-SNE Colored by True Labels')\n",
        "axes[0].set_xlabel('t-SNE 1')\n",
        "axes[0].set_ylabel('t-SNE 2')\n",
        "axes[0].legend(title=\"True Classes\", loc=\"best\", fontsize='small', markerscale=1.5)\n",
        "\n",
        "# --- Plot 2: Cluster Assignments (Aligned to True Labels) ---\n",
        "for label in unique_labels:\n",
        "    idx = aligned_preds == label\n",
        "    axes[1].scatter(X_2d[idx, 0], X_2d[idx, 1], c=[color_dict[label]],\n",
        "                    label=class_name_dict[label], s=20)\n",
        "axes[1].set_title('t-SNE Colored by K-Means Clusters')\n",
        "axes[1].set_xlabel('t-SNE 1')\n",
        "axes[1].set_ylabel('t-SNE 2')\n",
        "axes[1].legend(title=\"Cluster to Class\", loc=\"best\", fontsize='small', markerscale=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nmiAijKB2ANy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}